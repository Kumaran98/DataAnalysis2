{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ccbfe85",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import os.path\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a473fadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "Depression = pd.read_csv(\"./b_depressed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e59e42ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survey_id                 0\n",
       "Ville_id                  0\n",
       "sex                       0\n",
       "Age                       0\n",
       "Married                   0\n",
       "Number_children           0\n",
       "education_level           0\n",
       "total_members             0\n",
       "gained_asset              0\n",
       "durable_asset             0\n",
       "save_asset                0\n",
       "living_expenses           0\n",
       "other_expenses            0\n",
       "incoming_salary           0\n",
       "incoming_own_farm         0\n",
       "incoming_business         0\n",
       "incoming_no_business      0\n",
       "incoming_agricultural     0\n",
       "farm_expenses             0\n",
       "labor_primary             0\n",
       "lasting_investment        0\n",
       "no_lasting_investmen     20\n",
       "depressed                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Depression.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3c46bf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "Depression[\"no_lasting_investmen\"] = Depression[\"no_lasting_investmen\"].fillna(Depression[\"no_lasting_investmen\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3166c5a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survey_id                0\n",
       "Ville_id                 0\n",
       "sex                      0\n",
       "Age                      0\n",
       "Married                  0\n",
       "Number_children          0\n",
       "education_level          0\n",
       "total_members            0\n",
       "gained_asset             0\n",
       "durable_asset            0\n",
       "save_asset               0\n",
       "living_expenses          0\n",
       "other_expenses           0\n",
       "incoming_salary          0\n",
       "incoming_own_farm        0\n",
       "incoming_business        0\n",
       "incoming_no_business     0\n",
       "incoming_agricultural    0\n",
       "farm_expenses            0\n",
       "labor_primary            0\n",
       "lasting_investment       0\n",
       "no_lasting_investmen     0\n",
       "depressed                0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Depression.isna().sum()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5ea8dbff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = Depression.drop('depressed',axis=1)\n",
    "y = Depression['depressed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb2ebcfb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 140 candidates, totalling 700 fits\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X,y,train_size=0.7,random_state=100)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "Model = RandomForestClassifier(random_state=100,n_jobs=-1)\n",
    "params = {'max_depth':[3,5,10,12,15],\n",
    "'min_samples_leaf':[60,70,80,90,100,120,130],\n",
    "'n_estimators':[200],\n",
    "'max_features': [0.05,0.1,0.15,0.2],\n",
    "'criterion': [\"gini\"]}\n",
    "grid_search = GridSearchCV(param_grid=params,estimator=Model,n_jobs=-1,verbose=1,scoring='recall')\n",
    "grid_search.fit(X_train,y_train)\n",
    "Model_best = grid_search.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffe92663",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"trained_model.pickle\",\"wb\") as file: \n",
    "    pickle.dump(Model_best, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ee62e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model loading\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists(\"trained_model.pickle\"):\n",
    "    print(\"Trained model loading\")\n",
    "    model = pickle.load(open(\"trained_model.pickle\",\"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "346f45f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8afb3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trees = 25\n",
    "depth = 5\n",
    "used_features_rate = 0.5\n",
    "\n",
    "\n",
    "def create_forest_model():\n",
    "    inputs = create_model_inputs()\n",
    "    features = encode_inputs(inputs)\n",
    "    features = layers.BatchNormalization()(features)\n",
    "    num_features = features.shape[1]\n",
    "\n",
    "    forest_model = NeuralDecisionForest(\n",
    "        num_trees, depth, num_features, used_features_rate, num_classes\n",
    "    )\n",
    "\n",
    "    outputs = forest_model(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "forest_model = create_forest_model()\n",
    "\n",
    "run_experiment(forest_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "71db02a6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Expected binary or unicode string, got <_io.BufferedReader name='trained_model.pickle'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m converter \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlite\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFLiteConverter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_saved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrained_model.pickle\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m tflite_model \u001b[38;5;241m=\u001b[39m converter\u001b[38;5;241m.\u001b[39mconvert()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\lite\\python\\lite.py:1789\u001b[0m, in \u001b[0;36mTFLiteConverterV2.from_saved_model\u001b[1;34m(cls, saved_model_dir, signature_keys, tags)\u001b[0m\n\u001b[0;32m   1786\u001b[0m   tags \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m([_tag_constants\u001b[38;5;241m.\u001b[39mSERVING])\n\u001b[0;32m   1788\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m context\u001b[38;5;241m.\u001b[39meager_mode():\n\u001b[1;32m-> 1789\u001b[0m   saved_model \u001b[38;5;241m=\u001b[39m \u001b[43m_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43msaved_model_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signature_keys:\n\u001b[0;32m   1791\u001b[0m   signature_keys \u001b[38;5;241m=\u001b[39m saved_model\u001b[38;5;241m.\u001b[39msignatures\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:800\u001b[0m, in \u001b[0;36mload\u001b[1;34m(export_dir, tags, options)\u001b[0m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(export_dir, os\u001b[38;5;241m.\u001b[39mPathLike):\n\u001b[0;32m    799\u001b[0m   export_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mfspath(export_dir)\n\u001b[1;32m--> 800\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mload_partial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptions\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mroot\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    801\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\load.py:905\u001b[0m, in \u001b[0;36mload_partial\u001b[1;34m(export_dir, filters, tags, options)\u001b[0m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tags \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tags, \u001b[38;5;28mset\u001b[39m):\n\u001b[0;32m    901\u001b[0m   \u001b[38;5;66;03m# Supports e.g. tags=SERVING and tags=[SERVING]. Sets aren't considered\u001b[39;00m\n\u001b[0;32m    902\u001b[0m   \u001b[38;5;66;03m# sequences for nest.flatten, so we put those through as-is.\u001b[39;00m\n\u001b[0;32m    903\u001b[0m   tags \u001b[38;5;241m=\u001b[39m nest\u001b[38;5;241m.\u001b[39mflatten(tags)\n\u001b[0;32m    904\u001b[0m saved_model_proto, debug_info \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 905\u001b[0m     \u001b[43mloader_impl\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse_saved_model_with_debug_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    907\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mlen\u001b[39m(saved_model_proto\u001b[38;5;241m.\u001b[39mmeta_graphs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[0;32m    908\u001b[0m     saved_model_proto\u001b[38;5;241m.\u001b[39mmeta_graphs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mHasField(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mobject_graph_def\u001b[39m\u001b[38;5;124m\"\u001b[39m)):\n\u001b[0;32m    909\u001b[0m   metrics\u001b[38;5;241m.\u001b[39mIncrementReadApi(_LOAD_V2_LABEL)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py:57\u001b[0m, in \u001b[0;36mparse_saved_model_with_debug_info\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m     44\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mparse_saved_model_with_debug_info\u001b[39m(export_dir):\n\u001b[0;32m     45\u001b[0m   \u001b[38;5;124;03m\"\"\"Reads the savedmodel as well as the graph debug info.\u001b[39;00m\n\u001b[0;32m     46\u001b[0m \n\u001b[0;32m     47\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;124;03m    parsed. Missing graph debug info file is fine.\u001b[39;00m\n\u001b[0;32m     56\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m---> 57\u001b[0m   saved_model \u001b[38;5;241m=\u001b[39m \u001b[43mparse_saved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m   debug_info_path \u001b[38;5;241m=\u001b[39m file_io\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m     60\u001b[0m       saved_model_utils\u001b[38;5;241m.\u001b[39mget_debug_dir(export_dir),\n\u001b[0;32m     61\u001b[0m       constants\u001b[38;5;241m.\u001b[39mDEBUG_INFO_FILENAME_PB)\n\u001b[0;32m     62\u001b[0m   debug_info \u001b[38;5;241m=\u001b[39m graph_debug_info_pb2\u001b[38;5;241m.\u001b[39mGraphDebugInfo()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\saved_model\\loader_impl.py:89\u001b[0m, in \u001b[0;36mparse_saved_model\u001b[1;34m(export_dir)\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;124;03m\"\"\"Reads the savedmodel.pb or savedmodel.pbtxt file containing `SavedModel`.\u001b[39;00m\n\u001b[0;32m     76\u001b[0m \n\u001b[0;32m     77\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;124;03m  IOError: If the file does not exist, or cannot be successfully parsed.\u001b[39;00m\n\u001b[0;32m     86\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     87\u001b[0m \u001b[38;5;66;03m# Build the path to the SavedModel in pbtxt format.\u001b[39;00m\n\u001b[0;32m     88\u001b[0m path_to_pbtxt \u001b[38;5;241m=\u001b[39m file_io\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[1;32m---> 89\u001b[0m     \u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mas_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompat\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath_to_str\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexport_dir\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m     90\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_bytes(constants\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PBTXT))\n\u001b[0;32m     91\u001b[0m \u001b[38;5;66;03m# Build the path to the SavedModel in pb format.\u001b[39;00m\n\u001b[0;32m     92\u001b[0m path_to_pb \u001b[38;5;241m=\u001b[39m file_io\u001b[38;5;241m.\u001b[39mjoin(\n\u001b[0;32m     93\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_bytes(compat\u001b[38;5;241m.\u001b[39mpath_to_str(export_dir)),\n\u001b[0;32m     94\u001b[0m     compat\u001b[38;5;241m.\u001b[39mas_bytes(constants\u001b[38;5;241m.\u001b[39mSAVED_MODEL_FILENAME_PB))\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\compat.py:85\u001b[0m, in \u001b[0;36mas_bytes\u001b[1;34m(bytes_or_text, encoding)\u001b[0m\n\u001b[0;32m     83\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m bytes_or_text\n\u001b[0;32m     84\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 85\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected binary or unicode string, got \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m\n\u001b[0;32m     86\u001b[0m                   (bytes_or_text,))\n",
      "\u001b[1;31mTypeError\u001b[0m: Expected binary or unicode string, got <_io.BufferedReader name='trained_model.pickle'>"
     ]
    }
   ],
   "source": [
    "converter = tf.lite.TFLiteConverter.from_saved_model(open(\"trained_model.pickle\",\"rb\"))\n",
    "tflite_model = converter.convert()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d78d565a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
