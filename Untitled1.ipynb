{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a5b6c021",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "97965c85",
   "metadata": {},
   "outputs": [],
   "source": [
    "Depression = pd.read_csv(\"./b_depressed.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c49246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = Depression.drop('depressed',axis=1)\n",
    "y = Depression['depressed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "21818986",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survey_id                 0\n",
       "Ville_id                  0\n",
       "sex                       0\n",
       "Age                       0\n",
       "Married                   0\n",
       "Number_children           0\n",
       "education_level           0\n",
       "total_members             0\n",
       "gained_asset              0\n",
       "durable_asset             0\n",
       "save_asset                0\n",
       "living_expenses           0\n",
       "other_expenses            0\n",
       "incoming_salary           0\n",
       "incoming_own_farm         0\n",
       "incoming_business         0\n",
       "incoming_no_business      0\n",
       "incoming_agricultural     0\n",
       "farm_expenses             0\n",
       "labor_primary             0\n",
       "lasting_investment        0\n",
       "no_lasting_investmen     20\n",
       "depressed                 0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Depression.head()\n",
    "Depression.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bcffec39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imputing missing values\n",
    "Depression[\"no_lasting_investmen\"] = Depression[\"no_lasting_investmen\"].fillna(Depression[\"no_lasting_investmen\"].mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0d5cc657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survey_id                  int64\n",
       "Ville_id                   int64\n",
       "sex                        int64\n",
       "Age                        int64\n",
       "Married                    int64\n",
       "Number_children            int64\n",
       "education_level            int64\n",
       "total_members              int64\n",
       "gained_asset               int64\n",
       "durable_asset              int64\n",
       "save_asset                 int64\n",
       "living_expenses            int64\n",
       "other_expenses             int64\n",
       "incoming_salary            int64\n",
       "incoming_own_farm          int64\n",
       "incoming_business          int64\n",
       "incoming_no_business       int64\n",
       "incoming_agricultural      int64\n",
       "farm_expenses              int64\n",
       "labor_primary              int64\n",
       "lasting_investment         int64\n",
       "no_lasting_investmen     float64\n",
       "depressed                  int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Depression.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2712b9f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Survey_id                  int64\n",
       "Ville_id                   int64\n",
       "sex                       object\n",
       "Age                        int64\n",
       "Married                   object\n",
       "Number_children            int64\n",
       "education_level            int64\n",
       "total_members              int64\n",
       "gained_asset               int64\n",
       "durable_asset              int64\n",
       "save_asset                 int64\n",
       "living_expenses            int64\n",
       "other_expenses             int64\n",
       "incoming_salary           object\n",
       "incoming_own_farm         object\n",
       "incoming_business         object\n",
       "incoming_no_business      object\n",
       "incoming_agricultural      int64\n",
       "farm_expenses              int64\n",
       "labor_primary             object\n",
       "lasting_investment         int64\n",
       "no_lasting_investmen     float64\n",
       "depressed                 object\n",
       "dtype: object"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Depression[\"sex\"] = Depression[ \"sex\"].astype(str)\n",
    "Depression[\"Married\"] = Depression[\"Married\"].astype(str)\n",
    "Depression[\"incoming_salary\"] = Depression[\"incoming_salary\"].astype(str)\n",
    "Depression[\"incoming_own_farm\"] = Depression[\"incoming_own_farm\"].astype(str)\n",
    "Depression[\"incoming_business\"] = Depression[\"incoming_business\"].astype(str)\n",
    "Depression[ \"incoming_no_business\"] = Depression[ \"incoming_no_business\"].astype(str)\n",
    "Depression[\"labor_primary\"] = Depression[\"labor_primary\"].astype(str)\n",
    "Depression[\"depressed\"] = Depression[\"depressed\"].astype(str)\n",
    "Depression.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "be6b95b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data,test_data = train_test_split(Depression,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "23ab9dcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = train_data[2:]\n",
    "test_data = test_data[2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "05cd32b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1141, 23)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data.head()\n",
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "878f0133",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_file = \"train_data.csv\"\n",
    "test_data_file = \"test_data.csv\"\n",
    "\n",
    "train_data.to_csv(train_data_file, index=False, header=False)\n",
    "test_data.to_csv(test_data_file, index=False, header=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "fd27f191",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV_HEADER = [\n",
    "  \"Age\",\n",
    "    \"Number_children\",\n",
    "    \"education_level\",\n",
    "    \"total_members\",\n",
    "    \"gained_asset\",\n",
    "     \"durable_asset\",\n",
    "    \"save_asset\",\n",
    "    \"living_expenses\",\n",
    "    \"other_expenses\",\n",
    "    \"incoming_agricultural\"\n",
    "     \"farm_expenses\",\n",
    "    \"lasting_investment\",\n",
    "    \"no_lasting_investmen\",\n",
    "    \"sex\",\n",
    "    \"Married\",\n",
    "    \"incoming_salary\",\n",
    "    \"incoming_own_farm\",\n",
    "    \"incoming_business\",\n",
    "    \"incoming_no_business\",\n",
    "    \"labor_primary\"\n",
    "    \"depressed\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8dbcec8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A list of the numerical feature names.\n",
    "NUMERIC_FEATURE_NAMES = [\n",
    "    \"Age\",\n",
    "    \"Number_children\",\n",
    "    \"education_level\",\n",
    "    \"total_members\",\n",
    "    \"gained_asset\",\n",
    "     \"durable_asset\",\n",
    "    \"save_asset\",\n",
    "    \"living_expenses\",\n",
    "    \"other_expenses\",\n",
    "    \"incoming_agricultural\"\n",
    "     \"farm_expenses\",\n",
    "    \"lasting_investment\",\n",
    "    \"no_lasting_investmen\"\n",
    "]\n",
    "# A dictionary of the categorical features and their vocabulary.\n",
    "CATEGORICAL_FEATURES_WITH_VOCABULARY = {\n",
    "    \"sex\": sorted(list(train_data[\"sex\"].unique())),\n",
    "    \"Married\": sorted(list(train_data[\"Married\"].unique())),\n",
    "    \"incoming_salary\": sorted(list(train_data[\"incoming_salary\"].unique())),\n",
    "    \"incoming_own_farm\": sorted(list(train_data[\"incoming_own_farm\"].unique())),\n",
    "    \"incoming_business\": sorted(list(train_data[\"incoming_business\"].unique())),\n",
    "    \"incoming_no_business\": sorted(list(train_data[\"incoming_no_business\"].unique())),\n",
    "    \"labor_primary\": sorted(list(train_data[\"labor_primary\"].unique()))\n",
    "}\n",
    "# A list of the columns to ignore from the dataset.\n",
    "IGNORE_COLUMN_NAMES = [\"Survey_id\"]\n",
    "IGNORE_COLUMN_NAMES = [\"Ville_id\"]\n",
    "# A list of the categorical feature names.\n",
    "CATEGORICAL_FEATURE_NAMES = list(CATEGORICAL_FEATURES_WITH_VOCABULARY.keys())\n",
    "# A list of all the input features.\n",
    "FEATURE_NAMES = NUMERIC_FEATURE_NAMES + CATEGORICAL_FEATURE_NAMES\n",
    "# A list of column default values for each feature.\n",
    "COLUMN_DEFAULTS = [\n",
    "    [0.0] if feature_name in NUMERIC_FEATURE_NAMES + IGNORE_COLUMN_NAMES else [\"NA\"]\n",
    "    for feature_name in CSV_HEADER\n",
    "]\n",
    "# The name of the target feature.\n",
    "TARGET_FEATURE_NAME = \"depressed\"\n",
    "# A list of the labels of the target features.\n",
    "TARGET_LABELS = [\"0\", \"1\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "d177e4d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eelapriya\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py:2446: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import StringLookup\n",
    "\n",
    "target_label_lookup = StringLookup(\n",
    "    vocabulary=TARGET_LABELS, mask_token=None, num_oov_indices=0\n",
    ")\n",
    "\n",
    "\n",
    "def get_dataset_from_csv(csv_file_path, shuffle=False, batch_size=128):\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        csv_file_path,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_HEADER,\n",
    "        column_defaults=COLUMN_DEFAULTS,\n",
    "        label_name=TARGET_FEATURE_NAME,\n",
    "        num_epochs=1,\n",
    "        header=False,\n",
    "        na_value=\"?\",\n",
    "        shuffle=shuffle,\n",
    "    ).map(lambda features, target: (features, target_label_lookup(target)))\n",
    "    return dataset.cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "b4fa8d12",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model_inputs():\n",
    "    inputs = {}\n",
    "    for feature_name in FEATURE_NAMES:\n",
    "        if feature_name in NUMERIC_FEATURE_NAMES:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.float32\n",
    "            )\n",
    "        else:\n",
    "            inputs[feature_name] = layers.Input(\n",
    "                name=feature_name, shape=(), dtype=tf.string\n",
    "            )\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c10cf9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_inputs(inputs):\n",
    "    encoded_features = []\n",
    "    for feature_name in inputs:\n",
    "        if feature_name in CATEGORICAL_FEATURE_NAMES:\n",
    "            vocabulary = CATEGORICAL_FEATURES_WITH_VOCABULARY[feature_name]\n",
    "            # Create a lookup to convert a string values to an integer indices.\n",
    "            # Since we are not using a mask token, nor expecting any out of vocabulary\n",
    "            # (oov) token, we set mask_token to None and num_oov_indices to 0.\n",
    "            lookup = StringLookup(\n",
    "                vocabulary=vocabulary, mask_token=None, num_oov_indices=0\n",
    "            )\n",
    "            # Convert the string input values into integer indices.\n",
    "            value_index = lookup(inputs[feature_name])\n",
    "            embedding_dims = int(math.sqrt(lookup.vocabulary_size()))\n",
    "            # Create an embedding layer with the specified dimensions.\n",
    "            embedding = layers.Embedding(\n",
    "                input_dim=lookup.vocabulary_size(), output_dim=embedding_dims\n",
    "            )\n",
    "            # Convert the index values to embedding representations.\n",
    "            encoded_feature = embedding(value_index)\n",
    "        else:\n",
    "            # Use the numerical features as-is.\n",
    "            encoded_feature = inputs[feature_name]\n",
    "            if inputs[feature_name].shape[-1] is None:\n",
    "                encoded_feature = tf.expand_dims(encoded_feature, -1)\n",
    "\n",
    "        encoded_features.append(encoded_feature)\n",
    "\n",
    "    encoded_features = layers.concatenate(encoded_features)\n",
    "    return encoded_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "b0baed96",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralDecisionTree(keras.Model):\n",
    "    def __init__(self, depth, num_features, used_features_rate, num_classes):\n",
    "        super(NeuralDecisionTree, self).__init__()\n",
    "        self.depth = depth\n",
    "        self.num_leaves = 2 ** depth\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        # Create a mask for the randomly selected features.\n",
    "        num_used_features = int(num_features * used_features_rate)\n",
    "        one_hot = np.eye(num_features)\n",
    "        sampled_feature_indicies = np.random.choice(\n",
    "            np.arange(num_features), num_used_features, replace=False\n",
    "        )\n",
    "        self.used_features_mask = one_hot[sampled_feature_indicies]\n",
    "\n",
    "        # Initialize the weights of the classes in leaves.\n",
    "        self.pi = tf.Variable(\n",
    "            initial_value=tf.random_normal_initializer()(\n",
    "                shape=[self.num_leaves, self.num_classes]\n",
    "            ),\n",
    "            dtype=\"float32\",\n",
    "            trainable=True,\n",
    "        )\n",
    "\n",
    "        # Initialize the stochastic routing layer.\n",
    "        self.decision_fn = layers.Dense(\n",
    "            units=self.num_leaves, activation=\"sigmoid\", name=\"decision\"\n",
    "        )\n",
    "\n",
    "    def call(self, features):\n",
    "        batch_size = tf.shape(features)[0]\n",
    "\n",
    "        # Apply the feature mask to the input features.\n",
    "        features = tf.matmul(\n",
    "            features, self.used_features_mask, transpose_b=True\n",
    "        )  # [batch_size, num_used_features]\n",
    "        # Compute the routing probabilities.\n",
    "        decisions = tf.expand_dims(\n",
    "            self.decision_fn(features), axis=2\n",
    "        )  # [batch_size, num_leaves, 1]\n",
    "        # Concatenate the routing probabilities with their complements.\n",
    "        decisions = layers.concatenate(\n",
    "            [decisions, 1 - decisions], axis=2\n",
    "        )  # [batch_size, num_leaves, 2]\n",
    "\n",
    "        mu = tf.ones([batch_size, 1, 1])\n",
    "\n",
    "        begin_idx = 1\n",
    "        end_idx = 2\n",
    "        # Traverse the tree in breadth-first order.\n",
    "        for level in range(self.depth):\n",
    "            mu = tf.reshape(mu, [batch_size, -1, 1])  # [batch_size, 2 ** level, 1]\n",
    "            mu = tf.tile(mu, (1, 1, 2))  # [batch_size, 2 ** level, 2]\n",
    "            level_decisions = decisions[\n",
    "                :, begin_idx:end_idx, :\n",
    "            ]  # [batch_size, 2 ** level, 2]\n",
    "            mu = mu * level_decisions  # [batch_size, 2**level, 2]\n",
    "            begin_idx = end_idx\n",
    "            end_idx = begin_idx + 2 ** (level + 1)\n",
    "\n",
    "        mu = tf.reshape(mu, [batch_size, self.num_leaves])  # [batch_size, num_leaves]\n",
    "        probabilities = keras.activations.softmax(self.pi)  # [num_leaves, num_classes]\n",
    "        outputs = tf.matmul(mu, probabilities)  # [batch_size, num_classes]\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9db71e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralDecisionForest(keras.Model):\n",
    "    def __init__(self, num_trees, depth, num_features, used_features_rate, num_classes):\n",
    "        super(NeuralDecisionForest, self).__init__()\n",
    "        self.ensemble = []\n",
    "        # Initialize the ensemble by adding NeuralDecisionTree instances.\n",
    "        # Each tree will have its own randomly selected input features to use.\n",
    "        for _ in range(num_trees):\n",
    "            self.ensemble.append(\n",
    "                NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
    "            )\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # Initialize the outputs: a [batch_size, num_classes] matrix of zeros.\n",
    "        batch_size = tf.shape(inputs)[0]\n",
    "        outputs = tf.zeros([batch_size, num_classes])\n",
    "\n",
    "        # Aggregate the outputs of trees in the ensemble.\n",
    "        for tree in self.ensemble:\n",
    "            outputs += tree(inputs)\n",
    "        # Divide the outputs by the ensemble size to get the average.\n",
    "        outputs /= len(self.ensemble)\n",
    "        return outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "dc8efe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "batch_size = 265\n",
    "num_epochs = 10\n",
    "hidden_units = [64, 64]\n",
    "\n",
    "\n",
    "def run_experiment(model):\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(),\n",
    "        metrics=[keras.metrics.SparseCategoricalAccuracy()],\n",
    "    )\n",
    "\n",
    "    print(\"Start training the model...\")\n",
    "    train_dataset = get_dataset_from_csv(\n",
    "        train_data_file, shuffle=True, batch_size=batch_size\n",
    "    )\n",
    "\n",
    "    model.fit(train_dataset, epochs=num_epochs)\n",
    "    print(\"Model training finished\")\n",
    "\n",
    "    print(\"Evaluating the model on the test data...\")\n",
    "    test_dataset = get_dataset_from_csv(test_data_file, batch_size=batch_size)\n",
    "\n",
    "    _, accuracy = model.evaluate(test_dataset)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b6008b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Eelapriya\\anaconda3\\lib\\site-packages\\numpy\\core\\numeric.py:2446: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  return bool(asarray(a1 == a2).all())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training the model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "`label_name` provided must be one of the columns: ['Age', 'Number_children', 'education_level', 'total_members', 'gained_asset', 'durable_asset', 'save_asset', 'living_expenses', 'other_expenses', 'incoming_agriculturalfarm_expenses', 'lasting_investment', 'no_lasting_investmen', 'sex', 'Married', 'incoming_salary', 'incoming_own_farm', 'incoming_business', 'incoming_no_business', 'labor_primarydepressed']. Received: depressed.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [49]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[0;32m     20\u001b[0m tree_model \u001b[38;5;241m=\u001b[39m create_tree_model()\n\u001b[1;32m---> 21\u001b[0m \u001b[43mrun_experiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtree_model\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [48]\u001b[0m, in \u001b[0;36mrun_experiment\u001b[1;34m(model)\u001b[0m\n\u001b[0;32m      9\u001b[0m model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m     10\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(learning_rate\u001b[38;5;241m=\u001b[39mlearning_rate),\n\u001b[0;32m     11\u001b[0m     loss\u001b[38;5;241m=\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlosses\u001b[38;5;241m.\u001b[39mSparseCategoricalCrossentropy(),\n\u001b[0;32m     12\u001b[0m     metrics\u001b[38;5;241m=\u001b[39m[keras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mSparseCategoricalAccuracy()],\n\u001b[0;32m     13\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mStart training the model...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 16\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43mget_dataset_from_csv\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrain_data_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mfit(train_dataset, epochs\u001b[38;5;241m=\u001b[39mnum_epochs)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel training finished\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36mget_dataset_from_csv\u001b[1;34m(csv_file_path, shuffle, batch_size)\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_dataset_from_csv\u001b[39m(csv_file_path, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m):\n\u001b[1;32m----> 9\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexperimental\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmake_csv_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcsv_file_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumn_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCSV_HEADER\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumn_defaults\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mCOLUMN_DEFAULTS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabel_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mTARGET_FEATURE_NAME\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     15\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m     17\u001b[0m \u001b[43m        \u001b[49m\u001b[43mna_value\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m?\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     18\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshuffle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m features, target: (features, target_label_lookup(target)))\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mcache()\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\data\\experimental\\ops\\readers.py:552\u001b[0m, in \u001b[0;36mmake_csv_dataset_v2\u001b[1;34m(file_pattern, batch_size, column_names, column_defaults, label_name, select_columns, field_delim, use_quote_delim, na_value, header, num_epochs, shuffle, shuffle_buffer_size, shuffle_seed, prefetch_buffer_size, num_parallel_reads, sloppy, num_rows_for_inference, compression_type, ignore_errors)\u001b[0m\n\u001b[0;32m    549\u001b[0m   column_names \u001b[38;5;241m=\u001b[39m [column_names[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m select_columns]\n\u001b[0;32m    551\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m label_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m label_name \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m column_names:\n\u001b[1;32m--> 552\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`label_name` provided must be one of the columns: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    553\u001b[0m                    \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcolumn_names\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabel_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    555\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfilename_to_dataset\u001b[39m(filename):\n\u001b[0;32m    556\u001b[0m   dataset \u001b[38;5;241m=\u001b[39m CsvDataset(\n\u001b[0;32m    557\u001b[0m       filename,\n\u001b[0;32m    558\u001b[0m       record_defaults\u001b[38;5;241m=\u001b[39mcolumn_defaults,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    564\u001b[0m       compression_type\u001b[38;5;241m=\u001b[39mcompression_type\n\u001b[0;32m    565\u001b[0m   )\n",
      "\u001b[1;31mValueError\u001b[0m: `label_name` provided must be one of the columns: ['Age', 'Number_children', 'education_level', 'total_members', 'gained_asset', 'durable_asset', 'save_asset', 'living_expenses', 'other_expenses', 'incoming_agriculturalfarm_expenses', 'lasting_investment', 'no_lasting_investmen', 'sex', 'Married', 'incoming_salary', 'incoming_own_farm', 'incoming_business', 'incoming_no_business', 'labor_primarydepressed']. Received: depressed."
     ]
    }
   ],
   "source": [
    "num_trees = 10\n",
    "depth = 10\n",
    "used_features_rate = 1.0\n",
    "num_classes = len(TARGET_LABELS)\n",
    "\n",
    "\n",
    "def create_tree_model():\n",
    "    inputs = create_model_inputs()\n",
    "    features = encode_inputs(inputs)\n",
    "    features = layers.BatchNormalization()(features)\n",
    "    num_features = features.shape[1]\n",
    "\n",
    "    tree = NeuralDecisionTree(depth, num_features, used_features_rate, num_classes)\n",
    "\n",
    "    outputs = tree(features)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    return model\n",
    "\n",
    "\n",
    "tree_model = create_tree_model()\n",
    "run_experiment(tree_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b75776",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
